{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+Q0BZ2NA2RIbx5BZRyHj3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Konstantinosil/Chortareas-Linardatos-Central-Bank-Credibility-Under-the-Shadow-Rate-Stance/blob/main/US_EA_UK_Sacrifice_Ratio%2C_Chortareas_%26_Linardatos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGmBMSJAvKVJ",
        "outputId": "fe875275-0689-4b80-9d8f-49ebe2b68fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== [Credibility Data] ===\n",
            "        date econ  cred_sc_OWN\n",
            "0 2008-01-01   EU    35.432621\n",
            "1 2008-01-01   UK    23.602223\n",
            "2 2008-01-01   US    45.712574\n",
            "3 2008-02-01   EU    24.052893\n",
            "4 2008-02-01   UK     0.000000\n",
            "5 2008-02-01   US     0.000000\n",
            "6 2008-03-01   EU    26.468545\n",
            "7 2008-03-01   UK     2.638283\n",
            "8 2008-03-01   US     0.000000\n",
            "9 2008-04-01   EU    26.211599\n",
            "\n",
            "=== [INF & GDP Panel Data] ===\n",
            "        date econ         CLI          GDP      INF  POLICY        SR  \\\n",
            "0 2008-01-01   EU  101.418597  2480679.800   89.650    3.00  3.876700   \n",
            "1 2008-01-01   UK  100.907600   489783.000   84.100    5.25  5.285628   \n",
            "2 2008-01-01   US  101.325800    16843.003  211.080    3.94  2.655856   \n",
            "3 2008-02-01   EU  101.251102  2480679.800   89.970    3.00  4.053112   \n",
            "4 2008-02-01   UK  100.603000   489783.000   84.600    5.25  5.288706   \n",
            "5 2008-02-01   US  101.193100    16843.003  211.693    2.98  2.137866   \n",
            "6 2008-03-01   EU  101.044951  2480679.800   90.850    3.00  4.012206   \n",
            "7 2008-03-01   UK  100.242700   489783.000   84.900    5.25  5.426140   \n",
            "8 2008-03-01   US  101.046600    16843.003  213.528    2.61  1.874529   \n",
            "9 2008-04-01   EU  100.786838  2469697.900   91.150    3.00  4.024553   \n",
            "\n",
            "         VOL  \n",
            "0  30.047100  \n",
            "1   0.314064  \n",
            "2  26.200000  \n",
            "3  28.691400  \n",
            "4   0.236338  \n",
            "5  26.540000  \n",
            "6  27.308300  \n",
            "7   0.259693  \n",
            "8  25.610000  \n",
            "9  20.878100  \n",
            "\n",
            "=== [Sacrifice Ratio (SR) Rolling Window] ===\n",
            "        date econ  SR_H6  SR_H12\n",
            "0 2008-01-01   US    NaN     NaN\n",
            "1 2008-02-01   US    NaN     NaN\n",
            "2 2008-03-01   US    NaN     NaN\n",
            "3 2008-04-01   US    NaN     NaN\n",
            "4 2008-05-01   US    NaN     NaN\n",
            "5 2008-06-01   US    NaN     NaN\n",
            "6 2008-07-01   US    NaN     NaN\n",
            "7 2008-08-01   US    NaN     NaN\n",
            "8 2008-09-01   US    NaN     NaN\n",
            "9 2008-10-01   US    NaN     NaN\n",
            "\n",
            "=== [Merged Data] ===\n",
            "        date econ  cred_sc_OWN  SR_H6  SR_H12  cred_lag3  cred_lag6  \\\n",
            "0 2008-01-01   EU    35.432621    NaN     NaN        NaN        NaN   \n",
            "1 2008-01-01   UK    23.602223    NaN     NaN        NaN        NaN   \n",
            "2 2008-01-01   US    45.712574    NaN     NaN        NaN        NaN   \n",
            "3 2008-02-01   EU    24.052893    NaN     NaN        NaN        NaN   \n",
            "4 2008-02-01   UK     0.000000    NaN     NaN        NaN        NaN   \n",
            "5 2008-02-01   US     0.000000    NaN     NaN        NaN        NaN   \n",
            "6 2008-03-01   EU    26.468545    NaN     NaN        NaN        NaN   \n",
            "7 2008-03-01   UK     2.638283    NaN     NaN        NaN        NaN   \n",
            "8 2008-03-01   US     0.000000    NaN     NaN        NaN        NaN   \n",
            "9 2008-04-01   EU    26.211599    NaN     NaN  35.432621        NaN   \n",
            "\n",
            "   cred_lag12  pi_level  \n",
            "0         NaN       NaN  \n",
            "1         NaN       NaN  \n",
            "2         NaN       NaN  \n",
            "3         NaN       NaN  \n",
            "4         NaN       NaN  \n",
            "5         NaN       NaN  \n",
            "6         NaN       NaN  \n",
            "7         NaN       NaN  \n",
            "8         NaN       NaN  \n",
            "9         NaN       NaN  \n",
            "\n",
            "=== [Regression Results Table] ===\n",
            "         model   H   L      coef        se         t      pval   N        R2\n",
            "0   pooled_HC3   6   3 -0.013359  0.007297 -1.830812  0.067129  60  0.071818\n",
            "1   pooled_HAC   6   3 -0.013359  0.011918 -1.120878  0.262340  60  0.071818\n",
            "2   pooled_HC3   6   6 -0.012919  0.006845 -1.887388  0.059108  60  0.075171\n",
            "3   pooled_HAC   6   6 -0.012919  0.011576 -1.116004  0.264421  60  0.075171\n",
            "4   pooled_HC3   6  12 -0.014086  0.007654 -1.840216  0.065737  60  0.087543\n",
            "5   pooled_HAC   6  12 -0.014086  0.012363 -1.139362  0.254552  60  0.087543\n",
            "6   pooled_HC3  12   3 -0.013599  0.004995 -2.722771  0.006474  60  0.098336\n",
            "7   pooled_HAC  12   3 -0.013599  0.010873 -1.250657  0.211060  60  0.098336\n",
            "8   pooled_HC3  12   6 -0.013113  0.005340 -2.455658  0.014063  60  0.103785\n",
            "9   pooled_HAC  12   6 -0.013113  0.010953 -1.197262  0.231205  60  0.103785\n",
            "10  pooled_HC3  12  12 -0.010961  0.005511 -1.988815  0.046722  60  0.086585\n",
            "11  pooled_HAC  12  12 -0.010961  0.011646 -0.941133  0.346637  60  0.086585\n",
            "\n",
            "=== [Regression Summaries] ===\n",
            "pooled_HC3_H6_L3: N=60 | R2=0.072 | b=-0.0134, SE=0.0073, t=-1.831, p=0.0671 [HC3]\n",
            "pooled_HAC_H6_L3: N=60 | R2=0.072 | b=-0.0134, SE=0.0119, t=-1.121, p=0.2623 [HAC maxlags=6]\n",
            "pooled_HC3_H6_L6: N=60 | R2=0.075 | b=-0.0129, SE=0.0068, t=-1.887, p=0.0591 [HC3]\n",
            "pooled_HAC_H6_L6: N=60 | R2=0.075 | b=-0.0129, SE=0.0116, t=-1.116, p=0.2644 [HAC maxlags=6]\n",
            "pooled_HC3_H6_L12: N=60 | R2=0.088 | b=-0.0141, SE=0.0077, t=-1.840, p=0.0657 [HC3]\n",
            "pooled_HAC_H6_L12: N=60 | R2=0.088 | b=-0.0141, SE=0.0124, t=-1.139, p=0.2546 [HAC maxlags=6]\n",
            "pooled_HC3_H12_L3: N=60 | R2=0.098 | b=-0.0136, SE=0.0050, t=-2.723, p=0.0065 [HC3]\n",
            "pooled_HAC_H12_L3: N=60 | R2=0.098 | b=-0.0136, SE=0.0109, t=-1.251, p=0.2111 [HAC maxlags=12]\n",
            "pooled_HC3_H12_L6: N=60 | R2=0.104 | b=-0.0131, SE=0.0053, t=-2.456, p=0.0141 [HC3]\n",
            "pooled_HAC_H12_L6: N=60 | R2=0.104 | b=-0.0131, SE=0.0110, t=-1.197, p=0.2312 [HAC maxlags=12]\n",
            "pooled_HC3_H12_L12: N=60 | R2=0.087 | b=-0.0110, SE=0.0055, t=-1.989, p=0.0467 [HC3]\n",
            "pooled_HAC_H12_L12: N=60 | R2=0.087 | b=-0.0110, SE=0.0116, t=-0.941, p=0.3466 [HAC maxlags=12]\n"
          ]
        }
      ],
      "source": [
        "%pip -q install statsmodels linearmodels openpyxl\n",
        "\n",
        "import warnings, logging, pathlib\n",
        "import numpy as np, pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from linearmodels.panel import PanelOLS\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
        "\n",
        "PATH_CRED = pathlib.Path('/content/credibility_planB.xlsx')\n",
        "PATH_DATA = pathlib.Path('/content/chapter 4 august 2025 1st block.xlsx')\n",
        "\n",
        "assert PATH_CRED.exists(), \"Λείπει credibility_planB.xlsx στο /content\"\n",
        "assert PATH_DATA.exists(), \"Λείπει chapter 4 ... 1st block.xlsx στο /content\"\n",
        "\n",
        "ECON        = ['US','EU','UK']\n",
        "START, END  = '2008-01-01','2022-01-01'\n",
        "HP_LAMBDA_M = 129_600\n",
        "H_LIST      = (6, 12)\n",
        "LAGS        = (3, 6, 12)\n",
        "\n",
        "MIN_ABS_DPI = 0.02\n",
        "WINSOR_SR   = True\n",
        "W_PCTS      = (0.01, 0.99)\n",
        "\n",
        "USE_TREND_PI = True\n",
        "TREND_K      = 12\n",
        "\n",
        "FORWARD_WINDOW = False\n",
        "\n",
        "USE_CONTROLS = True\n",
        "\n",
        "MIN_ENTITIES_FOR_FE = 2\n",
        "MIN_T_FOR_FE        = 3\n",
        "\n",
        "def parse_sheet(name):\n",
        "    key = {'shadow rates':'SR','policy rates':'POLICY','inflation':'INF',\n",
        "           'volatility':'VOL','composite leader indicator':'CLI'}\n",
        "    parts = name.split()\n",
        "    econ = parts[-1].upper()\n",
        "    head = ' '.join(parts[:-1]).lower()\n",
        "    for k,v in key.items():\n",
        "        if k in head: return v, econ\n",
        "    if head.startswith('rgdp'): return 'GDP', econ\n",
        "    return None, None\n",
        "\n",
        "def to_monthly(s: pd.Series) -> pd.Series:\n",
        "    s = s.copy()\n",
        "    if not isinstance(s.index, pd.DatetimeIndex):\n",
        "        s.index = pd.to_datetime(s.index)\n",
        "    f = (pd.infer_freq(s.index[:5]) or '').upper()\n",
        "    if f.startswith(('D','B')):\n",
        "        s = s.resample('MS').last()\n",
        "    elif f.startswith('Q'):\n",
        "        s = s.resample('QS').last().interpolate('linear').resample('MS').ffill()\n",
        "    else:\n",
        "        if not s.index.is_month_start.all():\n",
        "            s.index = s.index.to_period('M').to_timestamp('start')\n",
        "        s = s.asfreq('MS')\n",
        "    return s\n",
        "\n",
        "def ensure_ms(s: pd.Series) -> pd.Series:\n",
        "    s = pd.Series(s)\n",
        "    if not isinstance(s.index, pd.DatetimeIndex):\n",
        "        s.index = pd.to_datetime(s.index)\n",
        "    return s.asfreq('MS')\n",
        "\n",
        "def winsorize(s: pd.Series, p=(0.01,0.99)) -> pd.Series:\n",
        "    if s.dropna().empty: return s\n",
        "    lo, hi = s.quantile(p[0]), s.quantile(p[1])\n",
        "    return s.clip(lower=lo, upper=hi)\n",
        "\n",
        "def load_inf_gdp_panel(path: pathlib.Path, econ_list, start, end):\n",
        "    xls = pd.read_excel(path, sheet_name=None)\n",
        "    raw = {}\n",
        "    for sheet, df in xls.items():\n",
        "        var, econ = parse_sheet(sheet)\n",
        "        if var and econ in econ_list:\n",
        "            df = df.iloc[:, :2].copy()\n",
        "            df.columns = ['date', var]\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            s = to_monthly(df.set_index('date')[var])\n",
        "            raw[(econ,var)] = s.loc[str(start):str(end)]\n",
        "    panel = (pd.concat(raw, axis=1)\n",
        "               .swaplevel(0,1, axis=1).sort_index(axis=1)\n",
        "               .stack(level=1).rename_axis(['date','econ']).sort_index())\n",
        "    return panel\n",
        "\n",
        "def load_credibility(path: pathlib.Path, econ_list):\n",
        "    xl = pd.ExcelFile(path)\n",
        "    try_order = ['cred_scaled_main', 'cred_scaled_GJR', 'cred_scaled_decommon']\n",
        "    sheet = next((s for s in try_order if s in xl.sheet_names), xl.sheet_names[0])\n",
        "    df = pd.read_excel(path, sheet_name=sheet)\n",
        "    if 'date' in df.columns:\n",
        "        df['date'] = pd.to_datetime(df['date']); df = df.set_index('date')\n",
        "    else:\n",
        "        df.index = pd.to_datetime(df.iloc[:,0], errors='coerce'); df = df.set_index(df.columns[0])\n",
        "    out=[]\n",
        "    for e in econ_list:\n",
        "        cand = [c for c in df.columns if str(c).lower().startswith('cred_sc_') and e in str(c)]\n",
        "        pref = [c for c in cand if 'egarch' in c.lower()]\n",
        "        col  = pref[0] if pref else (cand[0] if cand else None)\n",
        "        if col is None:\n",
        "            cand2 = [c for c in df.columns if c == e]\n",
        "            col = cand2[0] if cand2 else None\n",
        "        if col is None:\n",
        "            logging.warning(f\"[{e}] δεν βρέθηκε στήλη credibility στο {sheet}\")\n",
        "            continue\n",
        "        s = df[col].rename('cred_sc_OWN').to_frame(); s['econ']=e; out.append(s)\n",
        "    cred = (pd.concat(out).reset_index().rename(columns={'index':'date'})\n",
        "              .set_index(['date','econ']).sort_index())\n",
        "    return cred[['cred_sc_OWN']]\n",
        "\n",
        "def hp_gap_from_gdp(gdp: pd.Series, lam=HP_LAMBDA_M) -> pd.Series:\n",
        "    s = ensure_ms(pd.Series(gdp).astype(float)).dropna()\n",
        "    lvl = s.replace([np.inf,-np.inf], np.nan).dropna().clip(lower=1e-8)\n",
        "    loglvl = np.log(lvl).interpolate('time')\n",
        "    try:\n",
        "        cycle, trend = sm.tsa.filters.hpfilter(loglvl, lamb=lam)\n",
        "        gap = cycle * 100.0\n",
        "    except Exception:\n",
        "        tr = loglvl.rolling(24, min_periods=12, center=True).mean()\n",
        "        gap = (loglvl - tr) * 100.0\n",
        "    return gap.reindex(s.index)\n",
        "\n",
        "def inflation_from_index(cpi: pd.Series, kind='yoy') -> pd.Series:\n",
        "    \"\"\"Return inflation in percentage points (YoY or MoM annualized).\"\"\"\n",
        "    cpi = ensure_ms(pd.Series(cpi).astype(float)).dropna()\n",
        "    l = np.log(cpi)\n",
        "    if kind == 'yoy':\n",
        "        pi = 100*(l - l.shift(12))\n",
        "    elif kind == 'mom_ann':\n",
        "        pi = 1200*(l - l.shift(1))\n",
        "    else:\n",
        "        raise ValueError(\"kind must be 'yoy' or 'mom_ann'\")\n",
        "    return pi\n",
        "\n",
        "def trend_ma(pi, k=12):\n",
        "    return pi.rolling(k, min_periods=k).mean()\n",
        "\n",
        "def rolling_SR_from_pi_gap(pi: pd.Series, gap: pd.Series, H: int,\n",
        "                           min_abs_dpi=MIN_ABS_DPI,\n",
        "                           forward: bool=False) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Sacrifice Ratio (SR).\n",
        "    Backward [t-H+1, t] (default) ή forward [t+1, t+H] αν forward=True.\n",
        "    SR = (sum of negative gaps, %-years) / |drop in inflation|,\n",
        "    κρατάμε μόνο disinflations (Δπ<0), αγνοούμε πολύ μικρούς παρονομαστές με min_abs_dpi.\n",
        "    \"\"\"\n",
        "    pi  = ensure_ms(pd.Series(pi).astype(float)).sort_index()\n",
        "    gap = ensure_ms(pd.Series(gap).astype(float)).sort_index()\n",
        "\n",
        "    if not forward:\n",
        "        cost_pm = gap.clip(upper=0).abs().rolling(H, min_periods=H).sum()\n",
        "        cost_py = cost_pm / 12.0\n",
        "        dpi = pi - pi.shift(H)\n",
        "    else:\n",
        "        cost_py = (gap.clip(upper=0).abs().shift(1)\n",
        "                        .rolling(H, min_periods=H).sum()) / 12.0\n",
        "        dpi = pi.shift(-H) - pi\n",
        "\n",
        "    mask = (dpi < -min_abs_dpi)\n",
        "    sr = (cost_py / dpi.abs()).where(mask)\n",
        "\n",
        "    if WINSOR_SR:\n",
        "        sr = winsorize(sr, W_PCTS)\n",
        "    return sr\n",
        "\n",
        "def standardize_panel_index(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if not isinstance(df.index, pd.MultiIndex):\n",
        "        raise ValueError(\"Expect MultiIndex index.\")\n",
        "    names = list(df.index.names)\n",
        "    econ_lv = 'econ' if 'econ' in names else names[0]\n",
        "    date_lv = 'date' if 'date' in names else names[1]\n",
        "    econ = df.index.get_level_values(econ_lv)\n",
        "    date = pd.to_datetime(df.index.get_level_values(date_lv))\n",
        "    new_index = pd.MultiIndex.from_arrays([econ, date], names=['econ','date'])\n",
        "    out = df.copy()\n",
        "    out.index = new_index\n",
        "    return out.sort_index()\n",
        "\n",
        "def pooled_ols_HC3(y: pd.Series, X: pd.DataFrame):\n",
        "    Xc = sm.add_constant(X, has_constant='add')\n",
        "    return sm.OLS(y, Xc, missing='drop').fit(cov_type='HC3')\n",
        "\n",
        "def pooled_ols_HAC(y: pd.Series, X: pd.DataFrame, maxlags: int):\n",
        "    Xc = sm.add_constant(X, has_constant='add')\n",
        "    return sm.OLS(y, Xc, missing='drop').fit(cov_type='HAC', cov_kwds={'maxlags': maxlags})\n",
        "\n",
        "def fe_safe(y: pd.Series, X: pd.DataFrame, bandwidth: int):\n",
        "    df = pd.concat([y, X], axis=1).dropna()\n",
        "    if df.empty or df.shape[0] <= df.shape[1] + 1:\n",
        "        return None, None\n",
        "\n",
        "    df = standardize_panel_index(df)\n",
        "    yv = df.iloc[:,0]; Xv = df.iloc[:,1:]\n",
        "\n",
        "    ent = df.index.get_level_values('econ').nunique()\n",
        "    T   = df.index.get_level_values('date').nunique()\n",
        "\n",
        "    if ent < MIN_ENTITIES_FOR_FE or T < MIN_T_FOR_FE:\n",
        "        logging.info(f\"FE skipped (entities={ent}, T={T})\")\n",
        "        return None, None\n",
        "\n",
        "    candidates = []\n",
        "    if ent >= MIN_ENTITIES_FOR_FE and T >= MIN_T_FOR_FE:\n",
        "        candidates.append(('FE_DK_tw', dict(entity_effects=True,  time_effects=True)))\n",
        "        candidates.append(('FE_DK_ew', dict(entity_effects=True,  time_effects=False)))\n",
        "\n",
        "        candidates.append(('FE_DK_twonly', dict(entity_effects=False, time_effects=True)))\n",
        "\n",
        "    for label, eff in candidates:\n",
        "        try:\n",
        "            mod = PanelOLS(yv, Xv, **eff)\n",
        "            res = mod.fit(cov_type='kernel', kernel='bartlett',\n",
        "                          bandwidth=bandwidth, drop_absorbed=True)\n",
        "            return res, label\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"{label} DK failed: {e}; trying robust\")\n",
        "            try:\n",
        "                mod = PanelOLS(yv, Xv, **eff)\n",
        "                res = mod.fit(cov_type='robust', drop_absorbed=True)\n",
        "                return res, label.replace('DK','robust')\n",
        "            except Exception as e2:\n",
        "                logging.warning(f\"{label} robust failed: {e2}\")\n",
        "                continue\n",
        "    return None, None\n",
        "\n",
        "panel = load_inf_gdp_panel(PATH_DATA, ECON, START, END)\n",
        "cred  = load_credibility(PATH_CRED, ECON)\n",
        "\n",
        "needed = set(['INF','GDP'])\n",
        "have   = set(panel.columns.get_level_values(0))\n",
        "missing = needed - have\n",
        "if missing:\n",
        "    raise KeyError(f\"Χρειάζονται στήλες {missing} στο Chapter 4 αρχείο.\")\n",
        "\n",
        "sr_dict = {}\n",
        "pi_panel = []\n",
        "\n",
        "for econ in ECON:\n",
        "    sub   = panel.xs(econ, level='econ').sort_index()\n",
        "    cpi_e = sub.get('INF', pd.Series(dtype=float))\n",
        "    gdp_e = sub.get('GDP', pd.Series(dtype=float))\n",
        "    if cpi_e.dropna().empty or gdp_e.dropna().empty:\n",
        "        continue\n",
        "\n",
        "    gap_e = hp_gap_from_gdp(gdp_e)\n",
        "    pi_e  = inflation_from_index(cpi_e, kind='yoy')\n",
        "    if USE_TREND_PI:\n",
        "        pi_e = trend_ma(pi_e, k=TREND_K)\n",
        "\n",
        "    tmp_pi = pi_e.to_frame('PI').copy()\n",
        "    tmp_pi['econ'] = econ\n",
        "    tmp_pi = tmp_pi.set_index('econ', append=True).reorder_levels(['date','econ']).sort_index()\n",
        "    pi_panel.append(tmp_pi)\n",
        "\n",
        "    for H in H_LIST:\n",
        "        sr = rolling_SR_from_pi_gap(pi_e, gap_e, H, min_abs_dpi=MIN_ABS_DPI, forward=FORWARD_WINDOW)\n",
        "        sr.name = f'SR_H{H}'\n",
        "        df_sr = sr.to_frame()\n",
        "        df_sr['econ'] = econ\n",
        "        df_sr = (df_sr.set_index('econ', append=True)\n",
        "                        .reorder_levels(['date','econ'])\n",
        "                        .sort_index())\n",
        "        sr_dict[(econ, H)] = df_sr\n",
        "\n",
        "sr_all = pd.concat(sr_dict.values(), axis=1) if sr_dict else pd.DataFrame()\n",
        "if not sr_all.empty:\n",
        "    sr_all = sr_all.loc[:, ~sr_all.columns.duplicated()]\n",
        "\n",
        "pi_all = pd.concat(pi_panel).sort_index() if pi_panel else pd.DataFrame()\n",
        "\n",
        "df = cred.copy()\n",
        "for H in H_LIST:\n",
        "    col = f'SR_H{H}'\n",
        "    if (not sr_all.empty) and (col in sr_all.columns):\n",
        "        df = df.join(sr_all[[col]], how='left')\n",
        "\n",
        "for L in LAGS:\n",
        "    df[f'cred_lag{L}'] = df.groupby(level='econ')['cred_sc_OWN'].shift(L)\n",
        "\n",
        "if USE_CONTROLS and not pi_all.empty:\n",
        "    df = df.join(pi_all[['PI']].rename(columns={'PI':'pi_level'}), how='left')\n",
        "\n",
        "for H in H_LIST:\n",
        "    col = f\"SR_H{H}\"\n",
        "    n_nonnull = int(df[col].notna().sum()) if col in df.columns else 0\n",
        "    logging.info(f\"[Coverage] Non-null {col}: {n_nonnull}\")\n",
        "    if col in df.columns:\n",
        "        tmp = df[[col]].copy()\n",
        "        g = tmp.groupby(level='econ')[col].apply(lambda s: s.notna().sum())\n",
        "        logging.info(f\"[Coverage per entity] {col}: \" + \", \".join([f\"{k}={int(v)}\" for k,v in g.items()]))\n",
        "\n",
        "summ_text = {}\n",
        "rows = []\n",
        "\n",
        "def safe_add_result(tag, res, varname, model_label=None):\n",
        "    if res is None:\n",
        "        summ_text[tag] = 'n/a'\n",
        "        return\n",
        "    try:\n",
        "        b  = float(res.params.get(varname, np.nan))\n",
        "        se = float((res.bse if hasattr(res,'bse') else res.std_errors).get(varname, np.nan))\n",
        "        t  = float((res.tvalues if hasattr(res,'tvalues') else res.tstats).get(varname, np.nan))\n",
        "        p  = float(res.pvalues.get(varname, np.nan))\n",
        "    except Exception:\n",
        "        b=se=t=p=np.nan\n",
        "    N  = int(res.nobs) if hasattr(res, 'nobs') else np.nan\n",
        "    R2 = float(getattr(res, 'rsquared', np.nan))\n",
        "    label = f\" [{model_label}]\" if model_label else \"\"\n",
        "    summ_text[tag] = f\"N={N} | R2={R2:.3f} | b={b:.4f}, SE={se:.4f}, t={t:.3f}, p={p:.4f}{label}\"\n",
        "\n",
        "for H in H_LIST:\n",
        "    ycol = f'SR_H{H}'\n",
        "    if ycol not in df.columns:\n",
        "        continue\n",
        "\n",
        "    for L in LAGS:\n",
        "        xname = f'cred_lag{L}'\n",
        "        cols = [ycol, xname]\n",
        "        if USE_CONTROLS and 'pi_level' in df.columns:\n",
        "            cols.append('pi_level')\n",
        "\n",
        "        use = df[cols].dropna()\n",
        "        if use.empty:\n",
        "            continue\n",
        "\n",
        "        ent = use.index.get_level_values('econ').nunique()\n",
        "        T   = use.index.get_level_values('date').nunique()\n",
        "        logging.info(f\"[H={H}, L={L}] after dropna: entities={ent}, T={T}, N={use.shape[0]}\")\n",
        "\n",
        "        X = use[[xname]].copy()\n",
        "        if USE_CONTROLS and 'pi_level' in use.columns:\n",
        "            X['pi_level'] = use['pi_level']\n",
        "\n",
        "        res_hc3 = pooled_ols_HC3(use[ycol], X)\n",
        "        rows.append(['pooled_HC3', H, L,\n",
        "                     res_hc3.params.get(xname, np.nan),\n",
        "                     res_hc3.bse.get(xname, np.nan),\n",
        "                     res_hc3.tvalues.get(xname, np.nan),\n",
        "                     res_hc3.pvalues.get(xname, np.nan),\n",
        "                     int(res_hc3.nobs), getattr(res_hc3,'rsquared',np.nan)])\n",
        "        safe_add_result(f'pooled_HC3_H{H}_L{L}', res_hc3, xname, 'HC3')\n",
        "\n",
        "        res_hac = pooled_ols_HAC(use[ycol], X, maxlags=H)\n",
        "        rows.append(['pooled_HAC', H, L,\n",
        "                     res_hac.params.get(xname, np.nan),\n",
        "                     res_hac.bse.get(xname, np.nan),\n",
        "                     res_hac.tvalues.get(xname, np.nan),\n",
        "                     res_hac.pvalues.get(xname, np.nan),\n",
        "                     int(res_hac.nobs), getattr(res_hac,'rsquared',np.nan)])\n",
        "        safe_add_result(f'pooled_HAC_H{H}_L{L}', res_hac, xname, f'HAC maxlags={H}')\n",
        "\n",
        "        res_fe, fe_label = fe_safe(use[ycol], X, bandwidth=max(6, H))\n",
        "        if res_fe is not None:\n",
        "            rows.append([fe_label, H, L,\n",
        "                         res_fe.params.get(xname, np.nan),\n",
        "                         res_fe.std_errors.get(xname, np.nan),\n",
        "                         res_fe.tstats.get(xname, np.nan),\n",
        "                         res_fe.pvalues.get(xname, np.nan),\n",
        "                         int(res_fe.nobs), getattr(res_fe,'rsquared',np.nan)])\n",
        "            safe_add_result(f'{fe_label}_H{H}_L{L}', res_fe, xname, fe_label)\n",
        "\n",
        "reg_table = pd.DataFrame(rows, columns=['model','H','L','coef','se','t','pval','N','R2'])\n",
        "\n",
        "print(\"\\n=== [Credibility Data] ===\")\n",
        "print(cred.reset_index().head(10))\n",
        "\n",
        "print(\"\\n=== [INF & GDP Panel Data] ===\")\n",
        "print(panel.reset_index().head(10))\n",
        "\n",
        "if not sr_all.empty:\n",
        "    print(\"\\n=== [Sacrifice Ratio (SR) Rolling Window] ===\")\n",
        "    print(sr_all.reset_index().head(10))\n",
        "else:\n",
        "    print(\"\\n=== [Sacrifice Ratio (SR) Rolling Window] ===\\n(κενό)\")\n",
        "\n",
        "print(\"\\n=== [Merged Data] ===\")\n",
        "print(df.reset_index().head(10))\n",
        "\n",
        "if not reg_table.empty:\n",
        "    print(\"\\n=== [Regression Results Table] ===\")\n",
        "    print(reg_table)\n",
        "else:\n",
        "    print(\"\\n=== [Regression Results Table] ===\\n(κενό)\")\n",
        "\n",
        "if summ_text:\n",
        "    print(\"\\n=== [Regression Summaries] ===\")\n",
        "    for key, val in summ_text.items():\n",
        "        print(f\"{key}: {val}\")\n",
        "else:\n",
        "    print(\"\\n=== [Regression Summaries] ===\\n(δεν υπάρχουν)\")\n"
      ]
    }
  ]
}